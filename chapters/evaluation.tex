\chapter{Evaluation}
\label{ch:evaluation}

How much data compression does the Hypertrie achieve due to applying the space reduction approach presented earlier? 
Does the new data structure compromise the overall efficiency, more specifically, the query processing speed? 
On the one side, this chapter describes the space-friendliness of Hypertrie by presenting the compression ratio achieved w.r.t. The original Hypertrie after the bulk loading with different RDF data sets is done for both Hypertrie variants.  
On the other side, we have a look at how the space enhanced Tentris can still maintain its proven efficiency. For that, I ran a series of benchmark experiments on real RDF data and queries. \\

As this work's effort is mainly toward enhancing the Tentris system in the context of space cost, all experiments consider only the original Tentris' hypertrie as the reference for comparison opting out other triple store systems. The experimental setup is described in section \ref{sec:exper_setup}. In the succeeding section \ref{sec:results}, the results are presented.

\section{Experimental Setup}
\label{sec:exper_setup}

\subsection{Data Sets and Queries}
\label{subsec:datasets}

 I used three different RDF data sets to assess the performance  (load time/ size) of the compressed Hypertrie. Concretely, I used Semantic Web Dog Food SWDF (372K triples) as well as the English DBpedia 2015-10 (1087 M triples) as real-world data sets. 
I also chose the 4 WatDiv synthetic data sets corresponding to scale factors (10, 100, 1000, 10000) \cite{WatDiv}. All three data sets were used during the size/load speed assessment. However, only the SWDF and DBpedia data set were incorporated into the benchmarks. As hypertrie currently support BGP-based SPARQL operations, the benchmark queries are limited to \verb|SELECT| queries with \verb|WHERE| and with or without \verb|DISTINCT| keywords. To generate a custom set of benchmark queries for DBpedia and SWDF, the FEASIBLE \cite{feasible} query generator framework was used with suitable configuration. The generated queries are based on real-world query logs contained in LSQ \cite{lsq}.

\subsection{Test Environment} I used the server machine "Geiser" provisioned and maintained by the data science research group at Paderborn university to run all the experiments. 
The server machine has two Xeon E5-2683 v4 CPUs and 512 GB of RAM. 
Geiser runs Ubuntu 18.04.4 LTS 64-bit on Linux Kernel 4.15.0-112-generic with Python 3.6.9 and OpenJDK Java 11.0.8 installed\footnote{The server specification may change in the future. However, by the time of the document submission date, the spec mentioned here holds.}.
The benchmark tool and the Tentris triple stores (compressed/ non-compressed) were installed locally on a single server instance to avoid network latency. 
Data sets held in N-Triples format (\verb|.nt| files) were uploaded to the server and stored on disk.

\subsection{Bulk Loading} 
Generally, bulk loading is the activity of loading a large volume of data into a data storage system (database, triple store, etc.) mainly in one call, thus in a relatively small amount of time. 
There are many use cases in which bulk loading could apply; one example is data import/ export.
In this work context, bulk loading is used to load an entire RDF graph into Tentris; thus, it can calculate its memory footprint and the time to load.
During the bulk loading experiment, all the three RDF datasets (Sec. \ref{subsec:datasets}) were involved for each variant of the triple stores (compressed, non-compressed).
I also expanded the load test to check the space utilization performance of the compressed Hypertrie as the dataset scales.
For that, I used generated WatDiv synthetic datasets with increasing scale factors: [10, 100, 1000, 10000]. 
The bulk loading tests' results were calculated using a set of utility methods inside the Tentris project. Those utilities rely on built-in functions inside C++ language, which interface low-level system services and data to their clients.

\subsubsection{Calculating Size}
There are many methods to calculate memory utilization of processes or data structures residents. 
Each method fits a set of use cases and they can vary with accuracy.
For the case at hand, I relied on the \verb|proc| virtual file system in the server instance that ran the experiments. Given that, "The \verb|proc| filesystem is a pseudo-filesystem which provides an
interface to kernel data structures.  It is commonly mounted at
\verb|/proc|.  
Typically, it is mounted automatically by the system, but it can also be mounted manually" \footnote{https://man7.org/linux/man-pages/man5/proc.5.html}. The content of the files inside \verb|proc| has mostely the status of \textit{read-only} as the files are written by the kernel. 
For each process (with a process id \textit{pid}) currently running in the linux system, there exists a subdirectory \verb|/proc/[|\textit{pid}\verb|]|. 
Underbeneath each subdirectory \verb|/proc/[|\textit{pid}\verb|]|, there are a set of files and subdirectories disclosing meta-information about process with \textit{pid}. The file \verb|/proc/[|\textit{pid}\verb|]/status| provides status information about the process including various memory consumption information. The information are provided in a human-readable fashion. \\

After a single bulk load experiment is finished, Tentris access the corresponding status file; i.e. \verb|/proc/slef/status| where the magic symbolic link \verb|self| is automatically evaluated to Tentris' process ID. The field \verb|VmRSS| inside \verb|Status| file provide the current memory utilization of the resident Tentris process including the sizes of resident anonymous memory, resident file mappings and resident shared memory. 

\subsubsection{Calculating Load Time}
A simple tic-toc approach is adopted to calculate the load speed of RDF graphs into Tentris. More specifically, two start and end time flags are setup immediately before and after the actual loading of keys. The difference between the time checkpoints is then calculated to capture the load speed.  \\

The time checkpoints calculation utilized \verb|steady_clock| instead of \verb|system_clock|.
As \verb|system_clock| talks periodically to machine clock to correct itself, it might make minor timing mistakes. As \verb|steady_clock| is independent from the system clock, thus providing more reliable timing insights. 
 
\subsection{Benchmark Execution}
For executing the benchmark, I used the generic SPARQL benchmark execution framework IGUANA v3.0.0-alpha2 \cite{conrads-2017-iguana-demo}. 
IGUANA is a benchmark suite for executing benchmarks. 
It takes a benchmark, namely a data set and a possible list of SPARQL queries/updates, as input. 
Then it simulates a SPARQL user that pushes a series of queries repeatedly in a stress test scenario to a SPARQL endpoint where the next request is sent immediately after returning the last response. 
IGUANA can execute both synthetic benchmarks and benchmarks based on real data.
As part of its execution, the suite returns information on the respective triple store's different behavioral aspects, such as query processing speed for each query and the query result's size. 
The framework enables different benchmark execution options and fashions (measure performance of triple stores under updates, parallel user requests, etc ...). 
As Tentris provides an HTTP-based SPARQL interface, I used the HTTP-based benchmark with one user as a benchmark setup. 
The suite returned the average response time for each query, and the query result's size to consider later for comparison.  \\

\section{Results}
\label{sec:results}
Table \ref{tab:result_table_size} shows the allocated memory size for Tentris resulted from bulk loading different data sets (Sec. \ref{subsec:datasets}) using compressed and non-compressed versions of hypertrie. From the table, we notice that the \textit{global} space allocated for Tentris that utilizes the compressed hypertrie is reduced four times compared to the original settings. The compression ration of value $CP  \backsimeq 4$ stays stable across all data set except for SWDF where $CP \backsimeq 3$. Additionally, the compression ratio remains intact when WatDiv data sets scales up. Figure \ref{fig:eval_size} visualizes the size results. In the figure, results are normalized by viewing the average size acquired by a triple in the corresponding data set to be store in either version of Tentris. Similarly, table \ref{tab:result_table_load_speed} presents the loading speed of data sets during the bulk loading. From the table, we can infer that using a compressed version of hypertrie to represents the RDF graph in the triple store always results in less loading time (approximately 1.3 times faster) except for the SWDF data set where loading is appr. ten times faster using compressed hypertrie. The average load time by a single triple in a data set is also depicted in figure \ref{fig:eval_speed} calculated in microseconds. Figure \ref{fig:benchmarks_graph} highlights the benchmark results using a logarithmic axis on the query result size dimension. From the results, we can infer a substantial degradation of query processing efficiency when adopting the compressed version of hypertrie. The degradation goes down to half for some queries with moderate query result size (between 10-1000 triple). The performance degradation is more severe with SWDF than with DBpedia.

\begin{table}[h]
	\setlength{\tabcolsep}{1ex}
	\begin{tabular}{lrrrrrl}
		\toprule
		Data set& \# tripls & size (kB) (Compressed) &size (kB) (non-Compressed)& CR\\
		\midrule
		WatDiv (scale=10) &  1097093& 	107148	& 422744& 	3.945 \\ % structuredness 0.027901741842416984
		WatDiv (scale=100) & 10985185&	1032960	&4191216&	4.057  \\ % structuredness
		WatDiv (scale=1000) & 109911677	&10289024&	41760552&	4.059 \\ 
		WatDiv (scale=10000) & 1098338594&	104048444	&413277780&	3.972\\ 
		DBpedia  & 1087348475	&97680540&	361838396&	3.704&\\ 
		SWDF  & 372397&	71956&	205092	&2.85\\ 
		\bottomrule
	\end{tabular}
	\caption{The size of the resultant triple store against data sets in \ref{subsec:datasets} when the compressed and non-compressed variants of hypertries are used. All the values are in kilo bytes (kB). CR is abbr. for the compression ratio.}
	\label{tab:result_table_size}
\end{table}

\begin{table}[h]
	\setlength{\tabcolsep}{1ex}
	\begin{tabular}{lrrrrl}
		\toprule
		Data set& \# tripls & load time (sec) (Compressed) & load time (sec) (non-Compressed) \\
		\midrule
		WatDiv (scale=10) &  1097093& 	2.0702	& 3.0998 \\ % structuredness 0.027901741842416984
		WatDiv (scale=100) & 10985185&	33.0395	& 47.0945  \\ % structuredness
		WatDiv (scale=1000) & 109911677	&421.0285	& 553.088 \\ 
		WatDiv (scale=10000)& 1098338594 & 4948.0489	& 8316.0049\\ 
		DBpedia  & 1087348475	&4032,0386& 	6181,0737\\ 
		SWDF  & 372397&	0.0935& 1.0345\\ 
		\bottomrule
	\end{tabular}
	\caption{Results of load speed calculation.}
	\label{tab:result_table_load_speed}
\end{table}

\begin{figure}
	\centering
	%\vspace{0.3in}
	\includegraphics[width=\textwidth]{figures/chapter5/size}
	\caption{The bar chart shows the average size (in kB) acquired by a single triple for each data set when it is stored in either variants of Hypertrie.}
	\label{fig:eval_size}
\end{figure}

\begin{figure}
	\centering
	%\vspace{0.3in}
	\includegraphics[width=\textwidth]{figures/chapter5/loadspeed}
	\caption{The average load time (in $\mu $s) of an RDF triple calculated during the bulk loading experiment.}
	\label{fig:eval_speed}
\end{figure}
\clearpage

\begin{figure}[h]
	\centering
	\subfloat[SWDF]{%
		\includegraphics[width=\textwidth]{figures/chapter5/SWDFBenchmarktriangle}
	}
	
	\subfloat[DBpedia]{%
	\includegraphics[width=\textwidth]{figures/chapter5/DBpediaBenchmarktriangle}%
	}
	
	\caption{Average Queries per Second (avg QpS) by size of the query result considering the two variants of Hypertrie implementations (compressed, non-compressed). Figure 5.1a shows the results of benchmarking against SWDF dataset and associated queries, while figure 5.1b shows  the results of benchmarking against DBpedia dataset.}
	\label{fig:benchmarks_graph}
\end{figure}

\clearpage